{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "from fuzzywuzzy import fuzz\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import pymongo\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "src_dir = os.path.join(os.getcwd(), '..', '..', 'src')\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "# helper functions\n",
    "from d02_processing.cleaning_signatures import sorted_signatures\n",
    "from d02_processing.cleaning_signatures import cleaned_signatures\n",
    "from d01_utils.mongo_cursor_creator import mongo_cursor\n",
    "\n",
    "# Load the \"autoreload\" extension\n",
    "%load_ext autoreload\n",
    "\n",
    "# reload modules so that as you change code in src, it gets loaded\n",
    "%autoreload\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open up a Mongo Connection and create a cursor\n",
    "myclient = pymongo.MongoClient(\"mongodb://127.0.0.1:27017/\")\n",
    "curlydb = myclient['Naturaly_Curly_db']\n",
    "curly_collection = curlydb['curly_profiles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "532489\n"
     ]
    }
   ],
   "source": [
    "# Access all the unique items and store them to a list\n",
    "query_1 = curly_collection.find({})\n",
    "test = []\n",
    "for x in query_1:\n",
    "    test.append(x['signature'])\n",
    "\n",
    "# This is the number of total entire in the database\n",
    "print(len(test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7961\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'My hair is naturally a combination of 3B/3C. No longer texturized. All natural! CG Routine: Devapoo, One condition or Elucence. Loving curl creams: AG Recoil, Mop Curl Cream, KMS Curl Cream and never without CK. Gels: HE SMU and KCCC with a little BRHG on top.',\n",
       " 'Hair type 3A/3B- very course, high density, low porosity \\nCurrent CG products:\\nDevaCurl Low Poo Shampoo;\\xa0 Curl Quencher Moisturizing Conditioner; Cocktail of Curl Keeper and DevaCurl Ulta Defining Gel; and Curl Quencher Moisturizing Gel\\nMy YouTube videos: Curly Hair Rescue: Lessons from a Baby Boomer\\xa0https://youtu.be/dBeXSTfm0uE Aging and Curly Hair: Lessons from a Baby Boomer\\xa0https//youtu.be/X5JdtUCxQPkOver 60!',\n",
       " '\"Things are exactly as they should be, all evidence to the contrary.\"\\n\\npassword = niner\\n\\n',\n",
       " '2b/c/low porosity/medium thick\\nCG since 5/24/11\\nCo-wash: Suave Naturals Coconut\\nConditioner: Biolage Conditioning Balm\\nLeave-In: KCKT\\nStyler: KCCC\\n',\n",
       " '3b\\nTexture: coarse + some medium',\n",
       " 'Never:love5: relaxed\\n[SIGPIC][/SIGPIC]\\nExperimenting in creating my own products. ',\n",
       " 'Brand new wavy redhead growing out years of short hair! 2b/2c around chin length now; medium texture, normal/low porosity, normal elasticity, high density.',\n",
       " '\\n\\n\\n\\n\\nBig Chop: 11/25/09\\n\\n\\nCottony/fluffy, medium/coarse, high density & porosity\\n\\n\\nSL - APL - BSB - BSL - MBL - WL - HL\\n\\n',\n",
       " 'APL Stetched \\nRandom 3c/4a/4b mix \\nBSL while curly. I\\'m going to get there. :afro: \\n\\n\"Maybe some women weren\\'t meant to be tamed, maybe they\\'re suppose to run wild until they find someone just as wild to run with.\" -\\nCarrie Bradshaw']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique = list(set(test))\n",
    "print(len(unique))\n",
    "unique[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " 3a 3b\n",
      " high porosity\n",
      " suave naturals\n",
      " wash suave\n",
      " naturals coconut\n",
      " suave naturals coconut\n",
      " normal porosity\n",
      " wash suave naturals\n",
      " medium texture\n",
      " texture normal\n",
      "Cluster 1:\n",
      " fotki com\n",
      " public fotki\n",
      " public fotki com\n",
      " http public fotki com\n",
      " http public fotki\n",
      " http public\n",
      " sigpic sigpic\n",
      " 3b 3c\n",
      " http www\n",
      " blogspot com\n",
      "Cluster 2:\n",
      " 3c 4a\n",
      " shoulder length\n",
      " type 3c 4a\n",
      " type 3c\n",
      " hair type 3c 4a\n",
      " hair type 3c\n",
      " 3b 3c 4a\n",
      " hair type\n",
      " 3b 3c\n",
      " fotki com\n",
      "Cluster 3:\n",
      " 2c 3a\n",
      " 3a fine\n",
      " 2c 3a fine\n",
      " type 2c 3a\n",
      " type 2c\n",
      " low porosity\n",
      " 2c 3a 3b\n",
      " high porosity\n",
      " 2c 3a low\n",
      " 2c 3a medium\n",
      "Cluster 4:\n",
      " 3a ii\n",
      " ck coil\n",
      " cg aug\n",
      " ck coil jam\n",
      " ii cg\n",
      " coil jam\n",
      " 17 2005\n",
      " think 2b 3a ii\n",
      " fia think\n",
      " fia think 2b\n",
      "Cluster 5:\n",
      " low poo\n",
      " modified cg\n",
      " straight hair\n",
      " deep fix\n",
      " low porosity\n",
      " shea moisture\n",
      " 3a 3b\n",
      " styling kccc\n",
      " normal porosity\n",
      " la looks\n",
      "Cluster 6:\n",
      " med porosity\n",
      " 4a fine\n",
      " dry hair\n",
      " low med\n",
      " hair wash\n",
      " dr bronner\n",
      " coconut oil\n",
      " low med porosity\n",
      " moisture maniac\n",
      " 3c 4a fine\n",
      "Cluster 7:\n",
      " hair type\n",
      " type 3b\n",
      " hair type 3b\n",
      " type 4a\n",
      " hair type 4a\n",
      " tresemme naturals\n",
      " hair type 3a\n",
      " type 3a\n",
      " hair type 2b\n",
      " type 2b\n",
      "Cluster 8:\n",
      " low porosity\n",
      " low porosity medium\n",
      " porosity medium\n",
      " high density\n",
      " fine low\n",
      " fine low porosity\n",
      " low porosity high\n",
      " medium density\n",
      " porosity high\n",
      " low porosity high density\n",
      "Cluster 9:\n",
      " shampoo free\n",
      " april 2006\n",
      " shampoo free beginning april 2006\n",
      " free beginning\n",
      " free beginning april 2006\n",
      " shampoo free beginning\n",
      " beginning april 2006\n",
      " beginning april\n",
      " free beginning april\n",
      " shampoo free beginning april\n",
      "\n",
      "\n",
      "Prediction\n",
      "[8]\n",
      "[8]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "documents = unique\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(2, 5))\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "true_k = 10\n",
    "model = KMeans(n_clusters=true_k, init='random', max_iter=200, n_init=1)\n",
    "model.fit(X)\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind]),\n",
    "    print\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Prediction\")\n",
    "\n",
    "Y = vectorizer.transform([\"2a wavy low porosity\"])\n",
    "prediction = model.predict(Y)\n",
    "print(prediction)\n",
    "\n",
    "Y = vectorizer.transform([\"4c low porosity medium/fine density thick, ecoslay marmalade\"])\n",
    "prediction = model.predict(Y)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in unique:\n",
    "    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"\\n\", \" \", s)\n",
    "    s = re.sub(r\"med\\s\", \"medium \", s)\n",
    "    s = re.sub(r\"hi\\s\", \"high \", s)\n",
    "    s = re.sub(r\"po\\s\", \"porosity \", s)\n",
    "    s = re.sub(r\"den\\s\", \"density \", s)\n",
    "    s = re.sub(r\"www\", \"\", s)\n",
    "    s = re.sub(r\"blog\", \"\", s)\n",
    "    s = re.sub(r\"naturallycurly\", \"\", s)\n",
    "    s = re.sub(r\"com\", \"\", s)\n",
    "    s = re.sub(r\"public\", \"\", s)\n",
    "    s = re.sub(r\"http\", \"\", s)\n",
    "    s = re.sub(r\"fotki\", \"\", s)\n",
    "    s = re.sub(r\"youtube\", \"\", s)\n",
    "    s = re.sub(r\"hair\", \"\", s)\n",
    "    s = re.sub(r\"trying\", \"\", s)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-18-fec5591e8882>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-fec5591e8882>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    stop_words_sigs = ['www', 'blog', 'naturallycurly', 'com', 'public', 'http', 'fotki', 'hair', 'trying', 'try', 'youtube', 'channel', 'anxiety', 'depression', 'audrey', 'hepburn']\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# look at stop words for things like 'be' got to be glued \n",
    "    stop_words_sigs = ['www', 'blog', 'naturallycurly', 'com', 'public', 'http', 'fotki', 'hair', 'trying', 'try', 'youtube', 'channel', 'anxiety', 'depression', 'audrey', 'hepburn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at stop words for things like 'be' got to be glued \n",
    "    stop_words_sigs = ['www', 'blog', 'naturallycurly', 'com', 'public', 'http', 'fotki', 'hair', 'trying', 'try', 'sigpic', 'youtube', 'channel', 'anxiety', 'depression', 'audrey', 'hepburn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop_words_sigs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-fa08f5e58f5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#len(text.ENGLISH_STOP_WORDS)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhair_stop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENGLISH_STOP_WORDS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words_sigs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhair_stop_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop_words_sigs' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text \n",
    "#len(text.ENGLISH_STOP_WORDS)\n",
    "hair_stop_words = text.ENGLISH_STOP_WORDS.union(stop_words_sigs)\n",
    "len(hair_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "documents = unique\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=hair_stop_words, ngram_range=(2, 5))\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "true_k = 10\n",
    "model = KMeans(n_clusters=true_k, init='random', max_iter=200, n_init=1)\n",
    "model.fit(X)\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind]),\n",
    "    print\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Prediction\")\n",
    "\n",
    "Y = vectorizer.transform([\"2a wavy low porosity\"])\n",
    "prediction = model.predict(Y)\n",
    "print(prediction)\n",
    "\n",
    "Y = vectorizer.transform([\"4c low porosity medium/fine density thick, ecoslay marmalade\"])\n",
    "prediction = model.predict(Y)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After getting clusters then maybe run the cleaning algorithm on each cluster \n",
    "# and then plot most common characteristics, do pattens emerge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nc = range(2, 20)\n",
    "\n",
    "kmeans = [KMeans(n_clusters=i) for i in Nc]\n",
    "\n",
    "kmeans\n",
    "\n",
    "score = [kmeans[i].fit(X).score(X) for i in range(len(kmeans))]\n",
    "\n",
    "score\n",
    "\n",
    "plt.plot(Nc,score)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Elbow Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
